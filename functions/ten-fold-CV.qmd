---
title: "10-Fold CV"
author: "Catalina Ca√±izares"
format:
  html:
    theme: cosmo
    css: styles.css
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)
```

![Source](https://www.tmwr.org/images/cover.png)

10.2.1 CROSS-VALIDATION

Cross-validation is a well established resampling method. While there are a number of variations, the most common cross-validation method is V-fold cross-validation. The data are randomly partitioned into V sets of roughly equal size (called the folds). For illustration, V = 3 is shown in Figure 10.2 for a data set of 30 training set points with random fold allocations. The number inside the symbols is the sample number.

![](https://www.tmwr.org/premade/three-CV.svg)

Figure 10.2: V-fold cross-validation randomly assigns data to folds

The color of the symbols in Figure 10.2 represents their randomly assigned folds. Stratified sampling is also an option for assigning folds (previously discussed in Section 5.1).

For three-fold cross-validation, the three iterations of resampling are illustrated in Figure 10.3. For each iteration, one fold is held out for assessment statistics and the remaining folds are substrate for the model. This process continues for each fold so that three models produce three sets of performance statistics.

![](https://www.tmwr.org/premade/three-CV-iter.svg) Figure 10.3: V-fold cross-validation data usage

When V = 3, the analysis sets are 2/3 of the training set and each assessment set is a distinct 1/3. The final resampling estimate of performance averages each of the V replicates.

Using V = 3 is a good choice to illustrate cross-validation, but it is a poor choice in practice because it is too low to generate reliable estimates. In practice, values of V are most often 5 or 10; we generally prefer 10-fold cross-validation as a default because it is large enough for good results in most situations.

The primary input is the training set data frame as well as the number of folds (defaulting to 10):

```{r, eval=FALSE}
set.seed(1001)
ames_folds <- vfold_cv(ames_train, v = 10)
ames_folds
```

The column named splits contains the information on how to split the data (similar to the object used to create the initial training/test partition). While each row of splits has an embedded copy of the entire training set, R is smart enough not to make copies of the data in memory.20 The print method inside of the tibble shows the frequency of each: \[2107/235\] indicates that about two thousand samples are in the analysis set and 235 are in that particular assessment set.

These objects also always contain a character column called id that labels the partition.21

To manually retrieve the partitioned data, the analysis() and assessment() functions return the corresponding data frames:

```{r, eval=FALSE}
# For the first fold:
ames_folds$splits[[1]] %>% analysis() %>% dim()
```

There are a variety of cross-validation variations; we'll go through the most important ones.

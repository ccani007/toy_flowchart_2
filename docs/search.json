[
  {
    "objectID": "nodes/yrbs.html",
    "href": "nodes/yrbs.html",
    "title": "Youth Risk Behavior Surveillance System YRBSS",
    "section": "",
    "text": "Youth Risk Behavior Surveillance System (YRBSS)\nThe Youth Risk Behavior Surveillance System (YRBSS) is a set of surveys that track behaviors that can lead to poor health in public school students grades 9 through 12.\nThe surveys are administered every other year.\nSome of the health-related behaviors and experiences monitored are:\n\nStudent demographics: sex, sexual identity, race and ethnicity, and grade\nYouth health behaviors and conditions: sexual, injury and violence, bullying, diet and physical activity, obesity, and mental health, including suicide\nSubstance use behaviors: electronic vapor product and tobacco product use, alcohol use, and other drug use\nStudent experiences: parental monitoring, school connectedness, unstable housing, and exposure to community violence a group of teens\n\nData is also collected on sex and sexual orientation. The survey is anonymous, and no identifying data are collected.\nWhat is the purpose of YRBSS?\nThe YRBSS was designed to:\n\nDetermine how often unhealthy behaviors occur\nAssess whether unhealthy behaviors increase, decrease, or stay the same over time\nProvide data at the national, state, territorial and freely associated state, tribal, and local levels\nProvide data comparing different groups of adolescents\nMonitor progress toward achieving the Healthy People Objectives and other program goals\n\nWhy is YRBSS important?\nYRBS results help monitor adolescent health behavior changes over time, identify emerging issues, and plan and evaluate programs to support the health of youth.\nYRBS data are used by health departments, educators, lawmakers, doctors, and community organizations to inform school and community programs, communications campaigns, and other efforts."
  },
  {
    "objectID": "functions/ten-fold-CV.html",
    "href": "functions/ten-fold-CV.html",
    "title": "10-Fold CV",
    "section": "",
    "text": "10.2.1 CROSS-VALIDATION\nCross-validation is a well established resampling method. While there are a number of variations, the most common cross-validation method is V-fold cross-validation. The data are randomly partitioned into V sets of roughly equal size (called the folds). For illustration, V = 3 is shown in Figure 10.2 for a data set of 30 training set points with random fold allocations. The number inside the symbols is the sample number.\n\nFigure 10.2: V-fold cross-validation randomly assigns data to folds\nThe color of the symbols in Figure 10.2 represents their randomly assigned folds. Stratified sampling is also an option for assigning folds (previously discussed in Section 5.1).\nFor three-fold cross-validation, the three iterations of resampling are illustrated in Figure 10.3. For each iteration, one fold is held out for assessment statistics and the remaining folds are substrate for the model. This process continues for each fold so that three models produce three sets of performance statistics.\n Figure 10.3: V-fold cross-validation data usage\nWhen V = 3, the analysis sets are 2/3 of the training set and each assessment set is a distinct 1/3. The final resampling estimate of performance averages each of the V replicates.\nUsing V = 3 is a good choice to illustrate cross-validation, but it is a poor choice in practice because it is too low to generate reliable estimates. In practice, values of V are most often 5 or 10; we generally prefer 10-fold cross-validation as a default because it is large enough for good results in most situations.\nThe primary input is the training set data frame as well as the number of folds (defaulting to 10):\n\nset.seed(1001)\names_folds <- vfold_cv(ames_train, v = 10)\names_folds\n\nThe column named splits contains the information on how to split the data (similar to the object used to create the initial training/test partition). While each row of splits has an embedded copy of the entire training set, R is smart enough not to make copies of the data in memory.20 The print method inside of the tibble shows the frequency of each: [2107/235] indicates that about two thousand samples are in the analysis set and 235 are in that particular assessment set.\nThese objects also always contain a character column called id that labels the partition.21\nTo manually retrieve the partitioned data, the analysis() and assessment() functions return the corresponding data frames:\n\n# For the first fold:\names_folds$splits[[1]] %>% analysis() %>% dim()\n\nThere are a variety of cross-validation variations; weâ€™ll go through the most important ones."
  }
]